# Chapter 1

Installation and introduction

# Chapter 2

Machine Learning, a category of compouting where programmes learn to preform tasks they are not explicitly programmed for.
Generally this falls it to a the category of predicting an output based on the inputs.

## Supervised

This is considered the base for of ML and involves taking information we know and training a model on it to predict outcomes
on new information. It involves labelling data e.g, this is picture is a cat.

Common term for this type of model is a classifier, e.g a is cat or is not cat classifier.

Inputs must be, `Observable, Recordable, Knowable`

## Unsupervised

This works in simular way but the training data is not labelled to start with. The algorithm has cluster and group the
data itself based on the data labels it has already,.

The output is often a series of cluster centers, these are then labeled by the user.


## Parametric and None-parametric Learning

Parametric - is a trial and error approach. Generally this has a set number of inputs
None-parametric -counting and probability, this has no limit of the number of inputs and it can expand.


The above terms can be part of supervised and unsupervised learning. for example:

`Supervised Parametric` Learning involves a set number of inputs (parametric) and a user can control the value of the
inputs weights (turning a knob) until the desired out come is reached. The learning algorithmn can update the knobs
(weights) based on weather the outcome was accurate or not. This is also a search algorithm as the trial and error is
really searching for the optimum knob settings.

Steps for Supervised Parametric learning:
1. Make a prediction based on inputs
2. Compare that prediction to the truth
3. Based on how much the prediction was out adjust the weights

This course will only work with parametric models.

# Chapter 3
## Video 1

When designing a network we need to know the shap of the network. This starts with inputs and outputs.

In this example we we will take 1 input and one output. This will have a weight to it, which is a parameter we can
adjust (a knob from the example above). We will take in a value for the input representing the average number of toes
in the team and the output will represent weather or not the team will win.

See notebook : `chptr3_first_nn.ipynb` for example.

Input - Something that is measured in the real world
Prediction - The out come of the NN
Learning - done by trying to make a prediction

```python
weight = 0.1
# This is input layer of 1 and outputlaer of 1
def neural_network(input, weight):
    prediction = input * weight
    return prediction

number_of_toes = [8.5, 9.5, 10, 9]
input = number_of_toes[0]
pred = neural_network(input, weight)

print(pred)
```


Summary: A NN is collecting of weights that you multiple by the input to produce a collection fo weights.

## Video 2
Think of the wights as a knowledge its a learned value that is only way for NN to remember how to process inputs.

Weight acts as a way to apply volumes or to scale an input. (because the weight is used in a multiplication calculation).

All NN have the following pattern, `prediction = input * weight` The only veration you get is the number of inputs and weights.

## Video 3 - Multiple inputs

We will now have 3 inputs with 3 different weights.

Code in note book has been updated.

### Dot product and weighted sums

Now that we are summing arrays together to produce a value we can start using some called a vector. Which is list of numbers ion python.

A typical vector function will act on each position of a vector and then sum up the result. This is element wise operation.
This means if we have 2 vectors. We take the items at position 0, and apply a mathematical function, then next position and
 so one. The values of each sum are accumulated. An example is element wise multiplication, this apples a multiple function to all the positions and sums.

the `w_sum` func is this tyoe of function.

A dot product function can be thought of as a way of looking at the relationship between 2 vectors.

Because we are multipling by wieghts or wights can be treadted as boolean values. a weight of 0, 0.5, 1 or -1 would like Or, AND Big, AND or NOT. This related to how the weights will scale up (or down) the input values.

Understanding this will allow someone to read the weights of a network and understand the sort of decisions being made.
It feels like weights start to act as a bunch if logic operators in a network. When a weight is 0, it basically means the
input is ignored. When a weight is high for example '2' it means that the input is valued higher over other inputs.

** Return to this and try understand it more**

Code
```

# This is input layer of 1 and outputlaer of 1
def neural_network(input, weight):
    prediction = w_sum(inputs, weights)
    return prediction


# dot product value, sum to vectors (arrays) and return a value
def w_sum(a, b):
    assert(len(a) == len(b))
    output = 0
    for i in range(len(a)):
        output += a[i] * b[i] # some inputs and weights and accumlate output
    return output

# Weights
weights = [0.1, 0.2, 0.3] # weights length must equal n inputs


# Data
toes = [8.5, 9.5, 10, 9]
wlrec = [0.65, 0.8, 0.8, 0.99]
nfans = [1.2, 1.5, 0.5, 1.0]


inputs = [number_of_toes[0], wlrec[0], nfans[0]]
pred = neural_network(inputs, weights)

print(pred)

```

#### With Numpy

This code can be re-written to use the numpy library to preform this operations more efficantly.


```
import numpy as np
# This is input layer of 1 and outputlaer of 1
def neural_network(input, weight):
    prediction = inputs.dot(weights)
    return prediction

# Weights
weights = [0.1, 0.2, 0.3] # weights length must equal n inputs


# Data
toes = np.array([8.5, 9.5, 10, 9])
wlrec = np.array([0.65, 0.8, 0.8, 0.99])
nfans = np.array([1.2, 1.5, 0.5, 1.0])


inputs = np.array([number_of_toes[0], wlrec[0], nfans[0]])
pred = neural_network(inputs, weights)

print(pred)
```


## Video 4 - Multiple outputs

I this section with will have new simple network. This one will take 1 input, the win - loss rate and predict 3 outcomes:
 1. Did they win or loss
 2. % of how hurt the team is
 3. % sad or happy


I this example we use 1 piece of data to predict 3 very different types of outcome.

The code below prints out = [0.195, 0.13, 0.59]

The input is multiplied by each weight and each result is added to the output vector.

```
## Section 4 of chapter 3 - multiple outputs


def neural_network(input, weights):
    pred = ele_mul(input, weights)
    return pred

# element wise multiplication
def ele_mul(number, vector):
    output = [0,0,0]
    assert(len(output) == len(vector))
    for i in range(len(vector)):
        output[i] = number * vector[i]
    return output

weights = [0.3, 0.2, 0.9]
wlrec = [0.65, 0.8, 0.8, 0.9]
input = wlrec[0]
pred = neural_network(input, weights)
print(pred)
```

One we have input to outputs function we can now consider multiple inputs to multiple outputs. For this to work we need
a weight for each input to each output. This means 3 inputs would require 9 weights for 3 outputs.

For this to occur we need to use a matrix which is 2d vector (array).

## Section 4 of chapter 3 - multiple outputs pt 2

The key thing here is understanding the relationship here benwee the weights, which represented as a matrix and the
inputs / outputs which are represented by flat vectors.

There are 3 inputs and 3 opits, there for the matrix has a matric row (vector) for each input, and a weight value for each output

weights
```python
weights = [
    [0.1, 0.1, -0.3], // row for input 0
    [0.1, 0.2, 0.0], // row for input 1
    [0.0, 1.3, 1.0], // row for input 2
    // a vector should be = len(outputs)
]
```

An example of 2 input x 4 output layer would look like this

```python
weights = [
    [0.1, 0.1, -0.3, 0.1],
    [0.1, 0.2, 0.0, 0.3],
]
```

The code at this point does not support this.

From the video. We will leverage the numpy library to use a lot of matrix support.

Note that outputs of one network maybe inputs of another. This is due to the factor a single layer network with one
set of weights may not be enough for a network to do complex tasks. To deal with this we this we can have hidden layers of nodes. which preform the sample vector matrix multiplication.

To achieve this second set of weights is required.

See the code for pt3

## Numpy and how it works for NN

Numpy is scientific maths library. It will help with Matrix, vector matric and element wise multiplication.
Eventually this course will only use numpy to do this.
